<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
  <title>MBA Programmer - Gregory Choi</title>
  <link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
  <script src="https://code.jquery.com/jquery-1.12.4.js"></script>
  <script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
</head>
  <body>
  <div id="menu"></div><BR>
  <div id="body">
  
<b>[About Bayes Theorem]</b><br />
<br />
Let's assume that there is a basket that has 10 marbles.<br />
We know it has blue marbles or orange marbles for sure, but we don't know how many of them are blue or orange.<br />
<br />
<img src="./emptybasket.jpg" width="30%" height="30%"/>
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
You are asked to guess the color of the marble when you pick up the marble from the basket.<br />
Your best guess is either choose blue or orange by flipping coin. As you might know it's not scientific, but this is the best way to guess.<br />
<br />
Here is the different situation. Now, you know that there are 9 blue marbles and only 1 orange marble.<br />
<br />
<div class="separator" style="clear: both; text-align: center;">
<a href="https://2.bp.blogspot.com/-reayax_Y2Qc/Vwv230AWeYI/AAAAAAAAFuo/8rJrbHgrMcsjciF8mWwHQPPUcBsU456TQ/s1600/marble01.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="200" src="https://2.bp.blogspot.com/-reayax_Y2Qc/Vwv230AWeYI/AAAAAAAAFuo/8rJrbHgrMcsjciF8mWwHQPPUcBsU456TQ/s200/marble01.png" width="164" /></a></div>
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
In this case, no one would dare to guess that the marble has orange color. We all know the Bayes theorem intuitively. Prof. Woonkyung Kim in Korea university called it as "the evolution of the probability"<br />
<br />
Let's mathematically organize the situation. When we don't have the information, the probability of picking up a blue marble is<br />
<br />
P(Blue) = 0.5. Just same as flipping coin and it turns out to be head or toss.<br />
<br />
However, if we have the information,<br />
<br />
P(Blue | Information) = 0.9.<br />
<br />
Bayes theorem is in your smartphone also. (By the way, your cell phone is a collection of the probability theorems) The digital converts all the information into either 1 or 0. (Encoding the information). And prepared to send the digital signal. One way to make a difference between 1 or 0 in the signal is to manipulate the power of signal. We can give 0 less power and 1 more power. The power diagram is like below.<br />
<br />
<div class="separator" style="clear: both; text-align: center;">
<a href="https://2.bp.blogspot.com/-tZvRO-Oe2YI/Vwv4EfWke_I/AAAAAAAAFu0/56X2ddujUDUELSIfFvGfFsZv-6LzQ67Uw/s1600/signal01.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="82" src="https://2.bp.blogspot.com/-tZvRO-Oe2YI/Vwv4EfWke_I/AAAAAAAAFu0/56X2ddujUDUELSIfFvGfFsZv-6LzQ67Uw/s320/signal01.png" width="320" /></a></div>
<br />
<br />
<br />
<br />
<br />
<br />
However, as soon as this signal goes through the air, this signal meets white noise, which could be generated by your conversation, light, or interruption of other electro-magnetic wave. When this signal gets to your cell phone, it ends up with the probability distribution like below.<br />
<br />
<div class="separator" style="clear: both; text-align: center;">
<a href="https://1.bp.blogspot.com/-rFz-Gb3w1Wg/Vwv4c6Rrd0I/AAAAAAAAFu4/Trno8zhPKoE41KUFe1L5GfZIjZb2OzxWA/s1600/signal02.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="201" src="https://1.bp.blogspot.com/-rFz-Gb3w1Wg/Vwv4c6Rrd0I/AAAAAAAAFu4/Trno8zhPKoE41KUFe1L5GfZIjZb2OzxWA/s640/signal02.png" width="640" /></a></div>
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
When we get the signal if it has a blue line power, we naturally assume that that signal means 0. When we get the signal if it has a red line power, we naturally assume that that signal means 1. We are going to apply same theory to allow the machine to learn the data. (By the way, there's slight possibility of the error. Shannon came up with the way to correct this error. He also contributed to the birth of Classification Tree model)<br />
<br />
<b>[Validating the distribution function]</b><br />
<br />
Let's figure out that our test data(iris) has the normal distribution. You can use the histogram that you learn in stock market class.<br />
<a href="http://www.mbaprogrammer.com/2016/03/getting-stock-volatility-in-r-getting.html">[Review histogram]</a><br />
<br />
We are going to use sepal length and sepal width only.<br />
As this is not our scope (machine learning), I put the code into the different post.<br />
<a href="http://www.mbaprogrammer.com/2016/04/multiple-histograms-in-r.html">[Getting histograms on sepal length and sepal width (Multiple histograms)]</a><br />
<br />
<div class="separator" style="clear: both; text-align: center;">
<a href="https://2.bp.blogspot.com/-sd51ke_IbOo/Vwwjz9QkokI/AAAAAAAAFvg/PwcgcuZUkf0UR4FC0nF1DHyk_8kRjiF_A/s1600/Histogram%2BMultiples.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="237" src="https://2.bp.blogspot.com/-sd51ke_IbOo/Vwwjz9QkokI/AAAAAAAAFvg/PwcgcuZUkf0UR4FC0nF1DHyk_8kRjiF_A/s320/Histogram%2BMultiples.png" width="320" /></a></div>
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
As you can see, like 0 or 1 case, it has each sub species has a normal distribution in terms of sepal length and width. We can conclude that we can apply Naive bayes to our data set. Please, keep in mind that it cannot be used in a categorical value like Male or Female. Only, CART has a capability of handling categorical values.<br />
<br />
<b>[Before Codes]</b><br />
In this case, you need to install e1071 package.<br />
<span style="background-color: yellow;">Install.packages("e1071")</span><br />
<br />
<b>[Codes]</b><br />
library("e1071")<br />
library("gmodels")<br />
<br />
normalize &lt;- function(x) {<br />
<span style="color: #38761d;">&nbsp; &nbsp; #In this case, like KNN case, it would be better to normalize the data.</span><br />
&nbsp; &nbsp; mean_x &lt;- mean(x)<br />
&nbsp; &nbsp; stdev_x &lt;- sd(x)*sqrt((length(x)-1)/(length(x)))<br />
&nbsp; <br />
&nbsp; &nbsp; num &lt;- x - mean_x<br />
&nbsp; &nbsp; denom &lt;- stdev_x<br />
&nbsp; &nbsp; return (num/denom)<br />
}<br />
<br />
iris &lt;- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE) <span style="color: #38761d;">#There are great sample data offered by UCI. Let's use this!</span><br />
<br />
names(iris) &lt;- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")<br />
<span style="color: #38761d;">#Normalize data. Again, it is necessary as we deal with the distribution function.</span><br />
<span style="color: #38761d;">#If it is not normalized, the distribution function could be distorted.</span><br />
iris_norm &lt;- as.data.frame(lapply(iris[1:4], normalize))<br />
<br />
set.seed(1234)<br />
ind &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))<br />
iris.training &lt;- iris[ind==1, 1:4]<br />
iris.trainingLabels &lt;- iris[ind==1, 5]<br />
<br />
iris.test &lt;- iris[ind==2, 1:4]<br />
iris.testLabels &lt;- iris[ind==2, 5]<br />
<br />
<span style="color: #38761d;"># Making normal distribution functions.</span><br />
<span style="color: #38761d;"># For naive bayes, it is more important to choose the right training data set than in other models.</span><br />
<span style="background-color: yellow;">fit&lt;-naiveBayes(iris.training, iris.trainingLabels)</span><br />
<br />
<span style="color: #38761d;"># split up the data into two sets as we did always.&nbsp;</span><br />
<span style="color: #38761d;"># Training: Help your computer build the right model</span><br />
<span style="color: #38761d;"># Test: Validate your data set whether or not it works well.</span><br />
iris_pred &lt;- predict(fit, iris.test)<br />
<br />
<span style="color: #38761d;">#Confusion Matrix =&gt; Assessing the machine learning model. I'll explain deeper at another post.</span><br />
CrossTable(x = iris.testLabels, y = iris_pred, prop.chisq=FALSE)<br />
<div>
<br /></div>
<div>
<br /></div>
<div>
<b>[Result &amp; Interpretation]</b></div>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://3.bp.blogspot.com/-gD0dOUnoHvY/VwxYQCgCmdI/AAAAAAAAFv0/yDW_BpctgrAP2pRmUaZ1YHWU6PgcCGAiwCLcB/s1600/Screen%2BShot%2B2016-04-11%2Bat%2B10.06.39%2BPM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="406" src="https://3.bp.blogspot.com/-gD0dOUnoHvY/VwxYQCgCmdI/AAAAAAAAFv0/yDW_BpctgrAP2pRmUaZ1YHWU6PgcCGAiwCLcB/s640/Screen%2BShot%2B2016-04-11%2Bat%2B10.06.39%2BPM.png" width="640" /></a></div>
<div>
<br /></div>
Again, what we need to pay attention to is diagonal direction. So, the accuracy of this model is 36(=10+11+16)/38 = 94.73%. It's not that bad.<br />
<br />
Finding a right model is very difficult. It varies from the data to the data. That's why datascientists exist. You have to have good eye to choose the right model to the right data that you have. If you need, you need to formulate the cost-effective data-gathering strategy. That's why the data-scientists get paid a lot in recent years.<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />

  <script src="../js/tools.js"></script>
  </div>
  </body>
</html>